{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91950088",
   "metadata": {},
   "source": [
    "# **Levenshtein distance as a Linguistic Distance Proxy**\n",
    "*Written by Petter Stangeland*\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081bc3d8",
   "metadata": {},
   "source": [
    "The Levenshtein distance is a metric for measuring how different two strings are by counting the minimum number of single-character edits required to transform one string into the other. These edits are insertions, deletions, and substitutions.\n",
    "\n",
    "$$\n",
    "\\text{lev}(a, b) = \\begin{cases} |a| \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\ \\ \\ \\  \\text{if} \\ |b| = 0, \\\\ \n",
    "|b| \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\ \\ \\ \\  \\text{if} \\ |a| = 0, \\\\ \n",
    "\\text{lev}(\\text{tail}(a), \\text{tail}(b)) \\qquad \\qquad \\qquad \\ \\text{if} \\ \\text{head}(a) = \\text{head}(b), \n",
    "\\\\ 1 + \\text{min}\\begin{cases}\\text{lev}(\\text{tail}(a), b)\n",
    "\\\\ \\text{lev}(a, \\text{tail}(b)) \\qquad \\qquad \\text{otherwise}\n",
    "\\\\ \\text{lev}(\\text{tail}(a), \\text{tail}(b)) \n",
    "\\end{cases}\\end{cases}\n",
    "$$\n",
    "\n",
    "where \n",
    "where the tail $\\text{tail}(x)$ of some string $x$ is a string of all but the first character of $x$ (i.e. $\\text{tail}(x_0x_1\\dots x_n)=x_1x_2 \\dots x_n$), and $\\text{head}(x)$ is the first character of $x$ (i.e. $\\text{head}(x_0x_1\\dots x_n)=x_0$).\n",
    "\n",
    "Originally developed by the Russian scientist Vladimir Levenshtein in 1965, this distance has become a fundamental concept in fields such as computational linguistics, natural language processing, and bioinformatics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e425b066",
   "metadata": {},
   "source": [
    "\n",
    "### *Example*\n",
    "The three classical examples given to showcase how the Levenshtein distance measures distance is given by the pair \"kitten\" and \"sitting with a Levenshtein distance of 3:\n",
    "\n",
    "1. lev(\"kitten\", \"sitting\"), d = 0\n",
    "2. kitten → sitten (substitution of \"s\" for \"k\"), d = 1\n",
    "3. sitten → sittin (substitution of \"i\" for \"e\"), d = 2\n",
    "4. sittin → sitting (insertion of \"g\" at the end) d = 3\n",
    "\n",
    "Thus, the result for lev(\"kitten\", \"sitting\")=3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d239cc",
   "metadata": {},
   "source": [
    "\n",
    "### *Implementation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c6750",
   "metadata": {},
   "source": [
    "There is a plethora of ways to implement the Levenshtein distance. Some of the most common ones are a recursive definition as described in the definition above, and an iterative way with using a matrix (also known as the Wagner–Fischer algorithm), and improved version of the interative method using only two matrix rows instead of the entire matrix. Each method improves upon the previous one in terms of either space or time complexity. Here, for simplicity, I have chosen to implement the iterative matrix method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "605023f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from unidecode import unidecode\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cce73a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(a, b):\n",
    "    m = len(a)\n",
    "    n = len(b)\n",
    "    \n",
    "    matrix = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n",
    "    \n",
    "    for i in range(m + 1):\n",
    "        matrix[i][0] = i\n",
    "        \n",
    "    for j in range(n + 1):\n",
    "        matrix[0][j] = j\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if a[i - 1] == b[j - 1]:\n",
    "                matrix[i][j] = matrix[i - 1][j - 1]\n",
    "            else:\n",
    "                matrix[i][j] = 1 + min(matrix[i][j - 1], matrix[i - 1][j], matrix[i - 1][j - 1])\n",
    "                \n",
    "    return matrix[m][n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3dbc0",
   "metadata": {},
   "source": [
    "Analysing the time and space complextiy we see the iterative matrix method uses both $\\mathcal{O}(mn)$ time and space complexity.\n",
    "\n",
    "Let's quickly confirm out example from earlier numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7f1949b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(\"kitten\", \"sitting\") = 3\n"
     ]
    }
   ],
   "source": [
    "a = \"kitten\"\n",
    "b = \"sitting\"\n",
    "d = levenshtein(a, b)\n",
    "print(f'len(\"{a}\", \"{b}\") = {d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37f6831",
   "metadata": {},
   "source": [
    "### *Application*\n",
    "While you can use many the Levenshtein distance for many smart things such as word autocompletion and autocorrection as done in this article: [DitigalOcean: Implementing The Levenshtein Distance for Word Autocompletion and Autocorrection](https://www.digitalocean.com/community/tutorials/implementing-levenshtein-distance-word-autocomplete-autocorrect). However, I want to do something different. Instead, I want to apply the Levenshtein distance to look at linguistic (which I find more interesting). Namely, I want to use the Levenshtein distance as a proxy for the linguistic distance. According to Wikipedia, \n",
    "\n",
    "*the linguistic distance is the measure of how different one language (or dialect) is from another. Although they lack a uniform approach to quantifying linguistic distance between languages, linguists apply the concept to a variety of linguistic contexts, such as second-language acquisition, historical linguistics, language-based conflicts, and the effects of language differences on trade.*\n",
    "\n",
    "It is exactly for this reason why I would like to use the Levenshtein distance as a, albeit crude, way to quantify the linguistic distance between languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b3a227",
   "metadata": {},
   "source": [
    "### *Setup*\n",
    "\n",
    "I will use the Levenshtein distance to compare 10 languages:\n",
    "\n",
    "1. German\n",
    "\n",
    "2. Norwegian\n",
    "\n",
    "3. English\n",
    "\n",
    "4. French\n",
    "\n",
    "5. Spanish\n",
    "\n",
    "6. Hindi\n",
    "\n",
    "7. Urdu\n",
    "\n",
    "8. Japanese\n",
    "\n",
    "9. Cantonese\n",
    "\n",
    "10. Mandarin\n",
    "\n",
    "I began by selecting a list of the 100 most commonly used nouns in the English language. This list was then translated into various target languages, and I computed the Levenshtein distance between corresponding words in each pair of languages. The overall \"distance\" between two languages was defined as the sum of the Levenshtein distances across all 100 word pairs.\n",
    "\n",
    "To enable consistent comparison across languages with different writing systems, such as Mandarin, Cantonese, Japanese, Hindi, and Urdu, I converted all text to widely recognized romanized forms, ensuring that every word was represented using the Latin alphabet. Additionally, to simplify tone and accent variation (e.g., tones marked in Mandarin Pinyin), I removed all diacritical marks, retaining only the base letters.\n",
    "\n",
    "\n",
    "I chose these languages because they are among the most spoken in the world (yes, Norwegian included...). Additionally, based on their language families, we can form some a priori hypotheses about how similar or different they will be from one another.\n",
    "\n",
    "Specifically, I expect the group of Western European languages German, Norwegian, English, French, and Spanish to be relatively similar to each other and quite different from the rest. Within this group, I anticipate French and Spanish to be especially close due to their shared Romance roots, German and Norwegian to be closely related as Germanic languages, and English to lie somewhere in between, sharing features with both Romance and Germanic languages.\n",
    "\n",
    "I also expect Hindi and Urdu to form a distinct group. These two are often regarded as standardized registers of the same language, collectively known as Hindustani, and should therefore show a high degree of similarity.\n",
    "\n",
    "Finally, I expect Mandarin, Cantonese, and Japanese to form another cluster. While Japanese is not genetically related to the Chinese languages, it has borrowed a significant amount of vocabulary from Classical Chinese, which may result in some similarities with Mandarin. However, I anticipate Mandarin and Cantonese to be much closer to each other than either is to Japanese.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32da6b5",
   "metadata": {},
   "source": [
    "First we combine all the different language .txt files into a single pandas Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d65d50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>german</th>\n",
       "      <th>norwegian</th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "      <th>spanish</th>\n",
       "      <th>hindi</th>\n",
       "      <th>urdu</th>\n",
       "      <th>japanese</th>\n",
       "      <th>cantonese</th>\n",
       "      <th>mandarin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zeit</td>\n",
       "      <td>tid</td>\n",
       "      <td>time</td>\n",
       "      <td>temps</td>\n",
       "      <td>tiempo</td>\n",
       "      <td>smy</td>\n",
       "      <td>waqt</td>\n",
       "      <td>jikan</td>\n",
       "      <td>sihgaan</td>\n",
       "      <td>shijian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jahr</td>\n",
       "      <td>år</td>\n",
       "      <td>year</td>\n",
       "      <td>année</td>\n",
       "      <td>año</td>\n",
       "      <td>vrs</td>\n",
       "      <td>saal</td>\n",
       "      <td>nen</td>\n",
       "      <td>nihn</td>\n",
       "      <td>nian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>menschen</td>\n",
       "      <td>mennesker</td>\n",
       "      <td>people</td>\n",
       "      <td>personnes</td>\n",
       "      <td>gente</td>\n",
       "      <td>loga</td>\n",
       "      <td>log</td>\n",
       "      <td>hitobito</td>\n",
       "      <td>yahn</td>\n",
       "      <td>renmen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weg</td>\n",
       "      <td>vei</td>\n",
       "      <td>way</td>\n",
       "      <td>chemin</td>\n",
       "      <td>forma</td>\n",
       "      <td>raastaa</td>\n",
       "      <td>rasta</td>\n",
       "      <td>hoho</td>\n",
       "      <td>louh</td>\n",
       "      <td>fangshi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tag</td>\n",
       "      <td>dag</td>\n",
       "      <td>day</td>\n",
       "      <td>jour</td>\n",
       "      <td>día</td>\n",
       "      <td>din</td>\n",
       "      <td>din</td>\n",
       "      <td>hi</td>\n",
       "      <td>yaht</td>\n",
       "      <td>tian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     german  norwegian english     french spanish    hindi   urdu  japanese  \\\n",
       "0      zeit        tid    time      temps  tiempo      smy   waqt     jikan   \n",
       "1      jahr         år    year      année     año      vrs   saal       nen   \n",
       "2  menschen  mennesker  people  personnes   gente     loga    log  hitobito   \n",
       "3       weg        vei     way     chemin   forma  raastaa  rasta      hoho   \n",
       "4       tag        dag     day       jour     día      din    din        hi   \n",
       "\n",
       "  cantonese mandarin  \n",
       "0  sihgaan   shijian  \n",
       "1     nihn      nian  \n",
       "2     yahn    renmen  \n",
       "3     louh   fangshi  \n",
       "4     yaht      tian  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_folder = \"words\"\n",
    "files = [pd.read_csv(file, on_bad_lines='skip', header=None) for file in glob.glob(os.path.join(files_folder,\"*txt\"))]\n",
    "\n",
    "column_names = [\"german\", \"norwegian\", \"japanese\", \"hindi\", \"french\", \"urdu\", \"cantonese\", \"english\", \"mandarin\", \"spanish\"]\n",
    "df = pd.concat(files, axis=1, ignore_index=False)\n",
    "df.columns = column_names\n",
    "df[\"mandarin\"] = df[\"mandarin\"].apply(unidecode)\n",
    "df[\"german\"] = df[\"german\"].apply(str.lower)\n",
    "df[\"cantonese\"] = df[\"cantonese\"].apply(unidecode)\n",
    "df[\"japanese\"] = df[\"japanese\"].apply(unidecode)\n",
    "ordered = [\"german\", \"norwegian\", \"english\", \"french\", \"spanish\", \"hindi\", \"urdu\", \"japanese\", \"cantonese\", \"mandarin\"]\n",
    "df = df.reindex(ordered, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6add09db",
   "metadata": {},
   "source": [
    "Then we compute the total Levenshtein distance for all the language pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60f4aff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>german</th>\n",
       "      <th>norwegian</th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "      <th>spanish</th>\n",
       "      <th>hindi</th>\n",
       "      <th>urdu</th>\n",
       "      <th>japanese</th>\n",
       "      <th>cantonese</th>\n",
       "      <th>mandarin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>german</th>\n",
       "      <td>0</td>\n",
       "      <td>335</td>\n",
       "      <td>371</td>\n",
       "      <td>469</td>\n",
       "      <td>464</td>\n",
       "      <td>554</td>\n",
       "      <td>517</td>\n",
       "      <td>526</td>\n",
       "      <td>686</td>\n",
       "      <td>574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norwegian</th>\n",
       "      <td>335</td>\n",
       "      <td>0</td>\n",
       "      <td>340</td>\n",
       "      <td>439</td>\n",
       "      <td>437</td>\n",
       "      <td>529</td>\n",
       "      <td>487</td>\n",
       "      <td>490</td>\n",
       "      <td>682</td>\n",
       "      <td>561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>english</th>\n",
       "      <td>371</td>\n",
       "      <td>340</td>\n",
       "      <td>0</td>\n",
       "      <td>361</td>\n",
       "      <td>366</td>\n",
       "      <td>517</td>\n",
       "      <td>460</td>\n",
       "      <td>466</td>\n",
       "      <td>665</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>french</th>\n",
       "      <td>469</td>\n",
       "      <td>439</td>\n",
       "      <td>361</td>\n",
       "      <td>0</td>\n",
       "      <td>379</td>\n",
       "      <td>573</td>\n",
       "      <td>536</td>\n",
       "      <td>535</td>\n",
       "      <td>718</td>\n",
       "      <td>603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spanish</th>\n",
       "      <td>464</td>\n",
       "      <td>437</td>\n",
       "      <td>366</td>\n",
       "      <td>379</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>504</td>\n",
       "      <td>500</td>\n",
       "      <td>687</td>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hindi</th>\n",
       "      <td>554</td>\n",
       "      <td>529</td>\n",
       "      <td>517</td>\n",
       "      <td>573</td>\n",
       "      <td>540</td>\n",
       "      <td>0</td>\n",
       "      <td>392</td>\n",
       "      <td>531</td>\n",
       "      <td>665</td>\n",
       "      <td>563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>urdu</th>\n",
       "      <td>517</td>\n",
       "      <td>487</td>\n",
       "      <td>460</td>\n",
       "      <td>536</td>\n",
       "      <td>504</td>\n",
       "      <td>392</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>656</td>\n",
       "      <td>564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>japanese</th>\n",
       "      <td>526</td>\n",
       "      <td>490</td>\n",
       "      <td>466</td>\n",
       "      <td>535</td>\n",
       "      <td>500</td>\n",
       "      <td>531</td>\n",
       "      <td>510</td>\n",
       "      <td>0</td>\n",
       "      <td>645</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cantonese</th>\n",
       "      <td>686</td>\n",
       "      <td>682</td>\n",
       "      <td>665</td>\n",
       "      <td>718</td>\n",
       "      <td>687</td>\n",
       "      <td>665</td>\n",
       "      <td>656</td>\n",
       "      <td>645</td>\n",
       "      <td>0</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mandarin</th>\n",
       "      <td>574</td>\n",
       "      <td>561</td>\n",
       "      <td>550</td>\n",
       "      <td>603</td>\n",
       "      <td>562</td>\n",
       "      <td>563</td>\n",
       "      <td>564</td>\n",
       "      <td>513</td>\n",
       "      <td>505</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          german norwegian english french spanish hindi urdu japanese  \\\n",
       "german         0       335     371    469     464   554  517      526   \n",
       "norwegian    335         0     340    439     437   529  487      490   \n",
       "english      371       340       0    361     366   517  460      466   \n",
       "french       469       439     361      0     379   573  536      535   \n",
       "spanish      464       437     366    379       0   540  504      500   \n",
       "hindi        554       529     517    573     540     0  392      531   \n",
       "urdu         517       487     460    536     504   392    0      510   \n",
       "japanese     526       490     466    535     500   531  510        0   \n",
       "cantonese    686       682     665    718     687   665  656      645   \n",
       "mandarin     574       561     550    603     562   563  564      513   \n",
       "\n",
       "          cantonese mandarin  \n",
       "german          686      574  \n",
       "norwegian       682      561  \n",
       "english         665      550  \n",
       "french          718      603  \n",
       "spanish         687      562  \n",
       "hindi           665      563  \n",
       "urdu            656      564  \n",
       "japanese        645      513  \n",
       "cantonese         0      505  \n",
       "mandarin        505        0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(columns = df.columns, index = df.columns)\n",
    "levenshtein_vec = np.vectorize(levenshtein)\n",
    "for col1 in df:\n",
    "    for col2 in df:\n",
    "        result_df[col1][col2] = sum(levenshtein_vec(df[col1], df[col2]))\n",
    "\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21366fb",
   "metadata": {},
   "source": [
    "### *Visualization*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43d69d7",
   "metadata": {},
   "source": [
    "Naturally, we see the distances are all symmetric and on the diagonal all equal 0. To get a better visual representation we convert it to a 2d numpy array and use matplotlib.pyplot's imshow to show the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8fd2971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "matrix = result_df.to_numpy(dtype=float)\n",
    "chars = [\"G\", \"N\", \"E\", \"F\", \"S\", \"H\", \"U\", \"J\", \"C\", \"M\"]\n",
    "\n",
    "def plot(chars, matrix):\n",
    "    plt.imshow(matrix, cmap=\"hot_r\")\n",
    "    plt.axis(\"off\")\n",
    "    x1, y1 = 0.275, 0.90\n",
    "    x2, y2 = 0.215, 0.85\n",
    "    for char in chars:\n",
    "        delta1 = 0.05\n",
    "        delta2 = 0.078\n",
    "        plt.text(x1, y1, char, weight='demibold', fontsize=14, transform=plt.gcf().transFigure)\n",
    "        plt.text(x2, y2, char, weight='demibold', fontsize=14, transform=plt.gcf().transFigure)\n",
    "        x1 += delta1\n",
    "        y2 -= delta2\n",
    "    \n",
    "    plt.show()\n",
    "plot(chars, matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4085f5b4",
   "metadata": {},
   "source": [
    "Here, the letters on the axes represent the first letter of each language. We can roughly see that the languages cluster into three groups as expected, but Japanese stands out as distinct from the others.\n",
    "\n",
    "We could leave it at that, but I want to delve deeper by addressing an obvious issue I have so far overlooked. As mentioned earlier, Mandarin, Cantonese, Japanese, Hindi, and Urdu all use writing systems different from the Latin alphabet, and Mandarin and Cantonese are tonal languages. In my view, these factors should influence how we measure the linguistic distances.\n",
    "\n",
    "To tackle this, I propose the following approach: from this point on, I interpret all distances from the perspective of an English speaker—that is, as a proxy for how difficult it would be for an English speaker to learn each language. Specifically, I multiply the distances for languages with non-Latin scripts by a script adjustment factor, $\\sigma$, and for tonal languages by a tonal adjustment factor, $\\tau$.\n",
    "\n",
    "In order to estimate $\\tau$ I used looked at the paper by (Wang et al., 1999) which showed that in test group of native speakers of American English had around a 69% accuracy in identifying the Chinese tones (commonly four tones), which I then used to estimate $\\tau$ as $\\tau = \\frac{1}{0.7} \\approx 1.43$.\n",
    "\n",
    "In order to estimate $\\sigma$ I looked at the Foreign Service Institues list of learning hours for different languages: In category 3 (~1000 hours) \"Hard Languages\" you find languages like Russian, Bengali, and Hebrew. In category 4 (~2000 hours) \"Super-hard languages\" you find languages like Japanese, Cantonese, and Mandarin. The average learning hours for category 1 and 2 is around 800 hours, giving multiplier 1.25 from category 1/2 to 3 and multiplier 2.5 for category 1/2 to 4. For simplicity I take $\\sigma$ to be their average $\\sigma \\approx 1.875$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3744cc",
   "metadata": {},
   "source": [
    "### *Adjusting for tones and script*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7ed2e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tonal adjusment\n",
    "tau = 1.42 # tonal factor\n",
    "N = len(chars)\n",
    "P = np.eye(N)\n",
    "P[-2:,-2:] *= tau\n",
    "matrix = P@matrix@P\n",
    "matrix[-2:,-2:] *= (1/tau)**2\n",
    "\n",
    "#Script adjustment\n",
    "sigma = 1.875\n",
    "P = np.eye(N)\n",
    "P[-5:,-5:] *= tau\n",
    "matrix = P@matrix@P\n",
    "matrix[-5:,-5:] *= (1/sigma)**2\n",
    "\n",
    "plot(chars, matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1354c13",
   "metadata": {},
   "source": [
    "### *Conclusion*\n",
    "\n",
    "In this notebook, I have implemented the iterative matrix method for computing the Levenshtein distance and applied it to explore linguistic distance between ten languages. By translating a set of common English nouns and computing pairwise Levenshtein distances between corresponding words, I constructed a basic quantitative measure of language similarity.\n",
    "\n",
    "To account for structural differences in writing systems and the presence of tones, factors that are not captured by the Levenshtein distance alone, I introduced adjustment factors based on language learning difficulty and tone perception among native English speakers. While this approach relies on several heavy simplifications and assumptions, the resulting clusters of languages aligned broadly with known linguistic families and expected relationships. Further refinements, such as a less crude and more realistic way of incorporating phonetic, tonal and script information could enhance the accuracy and interpretability of the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f1d510",
   "metadata": {},
   "source": [
    "### *Sources*\n",
    "\n",
    "* Wikipedia Contributors. (2019, June 11). Levenshtein distance. Wikipedia; Wikimedia Foundation. https://en.wikipedia.org/wiki/Levenshtein_distance Introduction to Levenshtein distance. (2023, July 16). \n",
    "* GeeksforGeeks. https://www.geeksforgeeks.org/introduction-to-levenshtein-distance/\n",
    "\n",
    "* ‌Gad, A. F. (2020, March 15). Implementing The Levenshtein Distance for Word Autocompletion and Autocorrection. Digitalocean.com; DigitalOcean. https://www.digitalocean.com/community/tutorials/implementing-levenshtein-distance-word-autocomplete-autocorrect\n",
    "\n",
    "* Wikipedia Contributors. (2023, July 16). Linguistic distance. Wikipedia; Wikimedia Foundation. https://en.wikipedia.org/wiki/Linguistic_distance\n",
    "\n",
    "* Wang, Y., Spence, M. M., Jongman, A., & Sereno, J. A. (1999). Training American listeners to perceive Mandarin tones. The Journal of the Acoustical Society of America, 106(6), 3649–3658. https://doi.org/10.1121/1.428217\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
